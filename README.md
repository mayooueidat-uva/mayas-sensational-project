# dp3-project3-code-repo
## how sensational!: using sentiment analysis to investigate correlations between development of communications technology, and the sensationalisation of news 
maya uwaydat
#### about 
in ninth grade civics class, i learned about how the advent of 24-hour television caused greater political polarisation in the united states and how social media echo chambers only exacerbated the disunity felt between people on opposite sides of the political spectrum. if news is increasingly polarised, and if [the development of communications technology](https://education.cfr.org/media/history-communications) have only aided in polarisation, then maybe even the most reliable news must stir the worst of people's emotions in order to compete for their attention.\
to investigate the relationship between technological evolution and sensationalisation, i conducted sentiment analysis on both headlines and snippets of news articles, using the new york times as a case study. note: sensationalisation is not polarisation; polarisation is more about planting seeds of division while sensationalisation is more about making something as eye-catching and/or shocking as possible. this still concerns me because, well...it becomes a reflection of how much the news actually serves its role of constructively informing the public. in high school, i did quite the bit of research about the symbiotic relationship between terrorism and journalism; terrorist attacks don't cause too many deaths, [but are overrepresented in news](https://ourworldindata.org/does-the-news-reflect-what-we-die-from), because news is a for-profit enterprise that needs to keep your attention and terrorism is morbidly entertaining to people. 
#### limitations
this study uses *only* the new york times and its api. at first, i was considering looking at the top 3 newspapers (nyt, wsj and wp); however, the washington post does not have an api. the new york times was chosen for easy use of its api, its long history, and the fact that it is the [number one most-subscribed newspaper](https://en.wikipedia.org/wiki/List_of_newspapers_in_the_United_States) in the united states. a more comprehensive analysis would have taken into account *multiple* different newspaper outlets, television, podcasts, and other media. a project of that scale is something for future me, maybe.\
i also did not use the full articles; the headlines and the snippets are used as a proxy for "the sensationalisation of news." while they *are* indeed what is competing most for viewers' attention, the actual contents of the news might still use fairly neutral language.
#### challenges with this project 
+ limitations of the nyt api itself. i'm only allowed to make 5 api calls a minute and 500 calls a day. that greatly restricted the volume of my data; additionally, it forced me to be frugal with how many times i test-ran the pipeline.
+ because i'm gathering data from over the course of a century, the pipeline takes *awhile* to run. like, give it a good 40-50 minutes. if there was some error in my code that prevented my data from being stored in a permanent database, i could lose a lot of precious time because i'd have to fetch it again. 
#### running the pipeline
Running this pipeline is quite simple; the entire pipeline is condensed into one prefect flow. all one must do is launch a virtual environment, ensure that all the required packages are installed, and run the python file. **NOTE:** you need a key from the new york times developer api, which you will pass into your virtual environment. you can get one for free signing up at https://developer.nytimes.com/ and upon creating an app (instructions for creating an app included on the website). 
#### tools in the pipeline 
+ prefect: allows for easy orchestration of tasks. the data had to be pulled from the api, transformed, and shoved through a sentiment analysis algorithm; furthermore, for the sake of more robust analysis, i had to create a line plot. prefect allows me to do this all with one file, all in one go. 
+ pandas dataframes: for easy storage of data from the api before transformation.
+ duckdb/sql: i needed a more permanent place to store data that i fetched from api calls so i wouldn't have to re-fetch everything any time i had to go back and re-edit my code. duckdb provided this structure. 
+ vader: vader is a sentiment analysis tool specialised for social media posts, but it has been used for news articles as well. further insight: [analysis of vader vs. textblob](https://jds-online.org/journal/JDS/article/1441/info)
